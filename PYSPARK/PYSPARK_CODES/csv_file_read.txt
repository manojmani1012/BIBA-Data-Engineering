CSV data with PySpark
Importing  the csv files by using below commands:
-
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Reading csv file').getOrCreate()
file=spark.read.load("path.csv", format="csv"") -(using the absolute file path to avoid error)
file.show()  - to show the top 20 rows of csv files.


Using withColumnRenamed()-Used to change the column name
Syntax :
file = file.withColumnRenamed("rme_size_grp","Grp_size")
file.show()


Using select -Selecting the particular column 
Syntax :
file = file.select('year','Grp_size','unit')
file.show()

Group by()-To group data in a DataFrame based on one or more columns, and then perform aggregate functions on each group.
Syntax :
from pyspark.sql import functions as F
file.groupBy(["unit","Grp_size"]).agg(F.sum("unit"),F.max("unit")).show()

Sorting()-To sort the rows in a DataFrame based on one or more columns. 
syntax:
file.sort("unit").show()


Filter()-To filter rows in a DataFrame based on a specified condition. 
syntax-file.filter((file.unit=='COUNT') & (file.year>2000)).show()


Joins()- DataFrames to combine data from different sources based on common columns.
Syntax
file=file.join(file,['Grp_size','unit'],how='left')
file.show()

Aggregation-aggregation operations on a DataFrame to summarize or compute statistics on the data(Min,max,count,avg)
syntax:
file.groupBy(["unit","Grp_size"]).agg(F.sum("unit"),F.min("unit")).show()




